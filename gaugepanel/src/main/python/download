import os
import json
import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
from time import sleep

BASE_URL = "https://www.minecraftinfo.com/idlist.htm"
ROOT_URL = "https://www.minecraftinfo.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}
TEXTURE_DIR = Path("gaugepanel/src/main/resources/textures/minecraft/textures")
ITEMS_JSON_PATH = Path("gaugepanel/src/main/resources/textures/minecraft/items.json")

TEXTURE_DIR.mkdir(parents=True, exist_ok=True)

def parse_item_page(url):
    try:
        res = requests.get(url, headers=HEADERS)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # Get Minecraft ID
        id_td = soup.find("td", string="ID Name:")
        if not id_td:
            return None
        mc_id = id_td.find_next_sibling("td").get_text(strip=True)
        full_id = f"minecraft:{mc_id}"

        # Get display name
        img = soup.select_one("div.smitem img")
        name = img["title"] if img and img.has_attr("title") else mc_id

        # Get image URL from background-image style
        mimg_div = soup.select_one("div.mimg[style]")
        image_url = None
        if mimg_div:
            match = re.search(r"url\((.*?)\)", mimg_div["style"])
            if match:
                image_url = urljoin(ROOT_URL, match.group(1))

        if not image_url:
            return None

        # Extract filename
        image_filename = image_url.split("/")[-1]
        image_path = TEXTURE_DIR / image_filename

        # Download image if not already present
        if not image_path.exists():
            img_data = requests.get(image_url, headers=HEADERS)
            img_data.raise_for_status()
            with open(image_path, "wb") as f:
                f.write(img_data.content)
            sleep(0.1)  # be kind to the server

        return {
            "name": name,
            "id": full_id,
            "image": image_filename
        }

    except Exception as e:
        print(f"Error processing {url}: {e}")
        return None

def scrape_all_items():
    res = requests.get(BASE_URL, headers=HEADERS)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    items = []
    tables = soup.find_all("table")[1:]  # skip legend

    for table in tables:
        for row in table.find_all("tr")[1:]:  # skip headers
            link = row.find("a", href=True)
            if not link:
                continue
            href = link["href"]
            if not href.endswith(".htm"):
                continue

            item_url = urljoin(ROOT_URL, href)
            print(f"‚è≥ Processing {item_url}")
            item_data = parse_item_page(item_url)

            if item_data:
                items.append(item_data)
                print(f"‚úÖ {item_data['name']} ({item_data['id']})")
            else:
                print(f"‚ö†Ô∏è Skipped {item_url}")

    return items

def save_items_json(items):
    with open(ITEMS_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(items, f, indent=2, ensure_ascii=False)
    print(f"\nüì¶ Saved {len(items)} items to {ITEMS_JSON_PATH}")

if __name__ == "__main__":
    all_items = scrape_all_items()
    save_items_json(all_items)
